{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import imageio\n",
    "\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Maximum\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize as imresize\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "RANDOM_STATE = 11\n",
    "\n",
    "CLASS = {\n",
    "    'Black-grass': 0,\n",
    "    'Charlock': 1,\n",
    "    'Cleavers': 2,\n",
    "    'Common Chickweed': 3,\n",
    "    'Common wheat': 4,\n",
    "    'Fat Hen': 5,\n",
    "    'Loose Silky-bent': 6,\n",
    "    'Maize': 7,\n",
    "    'Scentless Mayweed': 8,\n",
    "    'Shepherds Purse': 9,\n",
    "    'Small-flowered Cranesbill': 10,\n",
    "    'Sugar beet': 11\n",
    "}\n",
    "\n",
    "INV_CLASS = {\n",
    "    0: 'Black-grass',\n",
    "    1: 'Charlock',\n",
    "    2: 'Cleavers',\n",
    "    3: 'Common Chickweed',\n",
    "    4: 'Common wheat',\n",
    "    5: 'Fat Hen',\n",
    "    6: 'Loose Silky-bent',\n",
    "    7: 'Maize',\n",
    "    8: 'Scentless Mayweed',\n",
    "    9: 'Shepherds Purse',\n",
    "    10: 'Small-flowered Cranesbill',\n",
    "    11: 'Sugar beet'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture adapted from https://www.kaggle.com/miklgr500/keras-simple-model-0-97103-best-public-score \n"
    "# Dense layers set\n",
    "def dense_set(inp_layer, n, activation, drop_rate=0.):\n",
    "    dp = Dropout(drop_rate)(inp_layer)\n",
    "    dns = Dense(n)(dp)\n",
    "    bn = BatchNormalization(axis=-1)(dns)\n",
    "    act = Activation(activation=activation)(bn)\n",
    "    return act\n",
    "\n",
    "# Conv. layers set\n",
    "def conv_layer(feature_batch, feature_map, kernel_size=(3, 3),strides=(1,1), zp_flag=False):\n",
    "    if zp_flag:\n",
    "        zp = ZeroPadding2D((1,1))(feature_batch)\n",
    "    else:\n",
    "        zp = feature_batch\n",
    "    conv = Conv2D(filters=feature_map, kernel_size=kernel_size, strides=strides)(zp)\n",
    "    bn = BatchNormalization(axis=3)(conv)\n",
    "    act = LeakyReLU(1/10)(bn)\n",
    "    return act\n",
    "\n",
    "# simple model \n",
    "def get_model():\n",
    "    inp_img = Input(shape=(51, 51, 3))\n",
    "\n",
    "    # 51\n",
    "    conv1 = conv_layer(inp_img, 64, zp_flag=False)\n",
    "    conv2 = conv_layer(conv1, 64, zp_flag=False)\n",
    "    mp1 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv2)\n",
    "    # 23\n",
    "    conv3 = conv_layer(mp1, 128, zp_flag=False)\n",
    "    conv4 = conv_layer(conv3, 128, zp_flag=False)\n",
    "    mp2 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv4)\n",
    "    # 9\n",
    "    conv7 = conv_layer(mp2, 256, zp_flag=False)\n",
    "    conv8 = conv_layer(conv7, 256, zp_flag=False)\n",
    "    conv9 = conv_layer(conv8, 256, zp_flag=False)\n",
    "    mp3 = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(conv9)\n",
    "    # 1\n",
    "    # dense layers\n",
    "    flt = Flatten()(mp3)\n",
    "    ds1 = dense_set(flt, 128, activation='tanh')\n",
    "    out = dense_set(ds1, 12, activation='softmax')\n",
    "\n",
    "    model = Model(inputs=inp_img, outputs=out)\n",
    "    \n",
    "    # The first 50 epochs are used by Adam opt.\n",
    "    # Then 30 epochs are used by SGD opt.\n",
    "    \n",
    "    #mypotim = Adam(lr=2 * 1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    mypotim = SGD(lr=1 * 1e-1, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                   optimizer=mypotim,\n",
    "                   metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_callbacks(filepath, patience=5):\n",
    "    lr_reduce = ReduceLROnPlateau(monitor='val_acc', factor=0.1, epsilon=1e-5, patience=patience, verbose=1)\n",
    "    msave = ModelCheckpoint(filepath, save_best_only=True)\n",
    "    return [lr_reduce, msave]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I trained model about 12h on GTX 950.\n",
    "def train_model(img, target):\n",
    "    callbacks = get_callbacks(filepath='model_weight_SGD.hdf5', patience=6)\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='model_weight_Adam.hdf5')\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "                                                        img,\n",
    "                                                        target,\n",
    "                                                        shuffle=True,\n",
    "                                                        train_size=0.8,\n",
    "                                                        random_state=RANDOM_STATE\n",
    "                                                        )\n",
    "    gen = ImageDataGenerator(\n",
    "            rotation_range=360.,\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            zoom_range=0.3,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True\n",
    "    )\n",
    "    gmodel.fit_generator(gen.flow(x_train, y_train,batch_size=BATCH_SIZE),\n",
    "               steps_per_epoch=10*len(x_train)/BATCH_SIZE,\n",
    "               epochs=EPOCHS,\n",
    "               verbose=1,\n",
    "               shuffle=True,\n",
    "               validation_data=(x_valid, y_valid),\n",
    "               callbacks=callbacks)\n",
    "\n",
    "def test_model(img, label):\n",
    "    gmodel = get_model()\n",
    "    gmodel.load_weights(filepath='model_weight_SGD.hdf5')\n",
    "    prob = gmodel.predict(img, verbose=1)\n",
    "    pred = prob.argmax(axis=-1)\n",
    "    sub = pd.DataFrame({\"file\": label,\n",
    "                         \"species\": [INV_CLASS[p] for p in pred]})\n",
    "    sub.to_csv(\"sub.csv\", index=False, header=True)\n",
    "    return gmodel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize all image to 51x51 \n",
    "def img_reshape(img):\n",
    "    img = imresize(img, (51, 51, 3))\n",
    "    return img\n",
    "\n",
    "# get image tag\n",
    "def img_label(path):\n",
    "    return str(str(path.split('/')[-1]))\n",
    "\n",
    "# get plant class on image\n",
    "def img_class(path):\n",
    "    return str(path.split('/')[-2])\n",
    "\n",
    "# fill train and test dict\n",
    "def fill_dict(paths, some_dict):\n",
    "    text = ''\n",
    "    if 'train' in paths[0]:\n",
    "        text = 'Start fill train_dict'\n",
    "    elif 'test' in paths[0]:\n",
    "        text = 'Start fill test_dict'\n",
    "\n",
    "    for p in tqdm(paths, ascii=True, ncols=85, desc=text):\n",
    "        img = imageio.imread(p)\n",
    "        img = img_reshape(img)\n",
    "        some_dict['image'].append(img)\n",
    "        some_dict['label'].append(img_label(p))\n",
    "        if 'train' in paths[0]:\n",
    "            some_dict['class'].append(img_class(p))\n",
    "\n",
    "    return some_dict\n",
    "\n",
    "# read image from dir. and fill train and test dict\n",
    "def reader():\n",
    "    file_ext = []\n",
    "    train_path = []\n",
    "    test_path = []\n",
    "\n",
    "    for root, dirs, files in os.walk('./input'):\n",
    "        if dirs != []:\n",
    "            print('Root:\\n'+str(root))\n",
    "            print('Dirs:\\n'+str(dirs))\n",
    "        else:\n",
    "            for f in files:\n",
    "                ext = os.path.splitext(str(f))[1][1:]\n",
    "\n",
    "                if ext not in file_ext:\n",
    "                    file_ext.append(ext)\n",
    "\n",
    "                if 'train' in root:\n",
    "                    path = os.path.join(root, f)\n",
    "                    train_path.append(path)\n",
    "                elif 'test' in root:\n",
    "                    path = os.path.join(root, f)\n",
    "                    test_path.append(path)\n",
    "    train_dict = {\n",
    "        'image': [],\n",
    "        'label': [],\n",
    "        'class': []\n",
    "    }\n",
    "    test_dict = {\n",
    "        'image': [],\n",
    "        'label': []\n",
    "    }\n",
    "\n",
    "    #train_dict = fill_dict(train_path, train_dict)\n",
    "    test_dict = fill_dict(test_path, test_dict)\n",
    "    \n",
    "    \n",
    "    return train_dict, test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root:\n",
      "./input\n",
      "Dirs:\n",
      "['test', 'train']\n",
      "Root:\n",
      "./input\\train\n",
      "Dirs:\n",
      "['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Start fill test_dict:   0%|                                  | 0/794 [00:00<?, ?it/s]C:\\Users\\dhruv\\Anaconda3\\lib\\site-packages\\skimage\\transform\\_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "Start fill test_dict: 100%|########################| 794/794 [00:11<00:00, 69.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 51, 51, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 49, 49, 64)        1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 49, 49, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)   (None, 49, 49, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 47, 47, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 47, 47, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)   (None, 47, 47, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)   (None, 21, 21, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 19, 19, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_76 (Batc (None, 19, 19, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_60 (LeakyReLU)   (None, 19, 19, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_77 (Batc (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_61 (LeakyReLU)   (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 5, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_78 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_62 (LeakyReLU)   (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 3, 3, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_79 (Batc (None, 3, 3, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)   (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_80 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 12)                1548      \n",
      "_________________________________________________________________\n",
      "batch_normalization_81 (Batc (None, 12)                48        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 12)                0         \n",
      "=================================================================\n",
      "Total params: 1,775,100\n",
      "Trainable params: 1,772,516\n",
      "Non-trainable params: 2,584\n",
      "_________________________________________________________________\n",
      "794/794 [==============================] - ETA: 35 - ETA: 24 - ETA: 20 - ETA: 18 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 18s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    train_dict, test_dict = reader()\n",
    "    #X_train = np.array(train_dict['image'])\n",
    "    #y_train = to_categorical(np.array([CLASS[l] for l in train_dict['class']]))\n",
    "\n",
    "    X_test = np.array(test_dict['image'])\n",
    "    label = test_dict['label']\n",
    "    \n",
    "    # I do not recommend trying to train the model on a kaggle.\n",
    "    #train_model(X_train, y_train)\n",
    "    return test_model(X_test, label)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    seednet = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import os\n",
    "\n",
    "def keras_to_tensorflow(keras_model, output_dir, model_name,out_prefix=\"output_\", log_tensorboard=True):\n",
    "\n",
    "    if os.path.exists(output_dir) == False:\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    out_nodes = []\n",
    "\n",
    "    for i in range(len(keras_model.outputs)):\n",
    "        out_nodes.append(out_prefix + str(i + 1))\n",
    "        tf.identity(keras_model.output[i], out_prefix + str(i + 1))\n",
    "\n",
    "    sess = K.get_session()\n",
    "\n",
    "    from tensorflow.python.framework import graph_util, graph_io\n",
    "\n",
    "    init_graph = sess.graph.as_graph_def()\n",
    "\n",
    "    main_graph = graph_util.convert_variables_to_constants(sess, init_graph, out_nodes)\n",
    "\n",
    "    graph_io.write_graph(main_graph, output_dir, name=model_name, as_text=False)\n",
    "\n",
    "    if log_tensorboard:\n",
    "        from tensorflow.python.tools import import_pb_to_tensorboard\n",
    "\n",
    "        import_pb_to_tensorboard.import_to_tensorboard(\n",
    "            os.path.join(output_dir, model_name),\n",
    "            output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 54 variables.\n",
      "INFO:tensorflow:Converted 54 variables to const ops.\n",
      "Model Imported. Visualize by running: tensorboard --logdir=C:\\Users\\dhruv\\Documents\\caltech\\plant-seedlings-classification\\checkpoint\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(os.getcwd(),\"checkpoint\")\n",
    "\n",
    "keras_to_tensorflow(seednet,output_dir=output_dir,model_name=\"seednet.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
